---
title: "Practical Machine Learning Course Project"
author: "Lim Zhen Xiong"
date: "May 23, 2015"
output: html_document
---

#Aim

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. This report uses data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. 

This report describes how the model was built, how cross validation was used, the expected out of sample error, and the justification for the choices made. The developed prediction model was also used to predict 20 different test cases. 

#Data Sandbox

The training data for this project are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

``` {r data}
trainUrl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

#creates "./data"" directory if it doesnt exists
if(!file.exists("./data")) {
        dir.create("./data")
}

#downloads training and testing data set if they dont already exist
if(!file.exists("./data/pml-training.csv")) {
        download.file(trainUrl, destfile="./data/pml-training.csv", method="curl")
}

if(!file.exists("./data/pml-testing.csv")) {
        download.file(trainUrl, destfile="./data/pml-testing.csv", method="curl")
}
```

#Data Cleansing

The raw training dataset consists of 19622 observations and 160 variables, while the raw test dataset consists of 20 observations and 160 variables. The datasets were cleaned to remove columns containing NA, columns that are not numeric (so that the prediction model can function properly) and unneeded column containing "X", "timestamp" and "window". Finally, the cleaned training dataset consists of 19622 observations and 53 variables, while the raw test dataset consists of 20 observations and 52 variables (less problem_id).


``` {r data cleansing}
library(caret)
trainRaw <- read.csv("./data/pml-training.csv")
testRaw <- read.csv("./data/pml-testing.csv")
dim(trainRaw); dim(testRaw)

#checking that Classe variable in training dataset doesnt contain NAs
sum(is.na(trainRaw$classe)) == 0

#removing columns containing NA
trainRawFilled <- trainRaw[,colSums(is.na(trainRaw)) == 0] 
testRawFilled <- testRaw[,colSums(is.na(testRaw)) == 0] 

#preserving Classe variable (type: numeric)
classe <- trainRawFilled$classe

#removing columns containing variables that are not numeric
trainNum <- trainRawFilled[,sapply(trainRawFilled, is.numeric) == 1]
testNum <- testRawFilled[,sapply(testRawFilled, is.numeric) == 1]

#removing unnecessary columns containing "X", "timestamp","window" and "id" in headers 
unCol1 <- grepl("X|timestamp|window", names(trainNum))
trainClean <- trainNum[,unCol1==0]
unCol2 <- grepl("X|timestamp|window|id", names(testNum))
testClean <- testNum[,unCol2==0]

#adding back classe variable to trainClean
trainClean$classe <- classe

# dimensions of cleaned training and testing data set
dim(trainClean); dim(testClean)
```

#Data Slicing

The cleaned training dataset was partitioned, with about 75% of the observations allocated in the training sub-dataset `myTraining` and the remaining in the testing sub-dataset `myTesting`. 

``` {r data slicing}
inTrain <- createDataPartition(y=trainClean$classe, p=0.75, list=FALSE)
myTraining <- trainClean[inTrain,]
myTesting <- trainClean[-inTrain,]
dim(myTraining)
```

There were no zero coviarates amongst the 52 variables remaining (less Classe which is the outcome), hence all 52 variables were included in the prediction algorithm.

``` {r covariates}
#checking if there are any zero covariates and if so, can be left out of prediction model
nsv <- nearZeroVar(myTraining, saveMetrics=TRUE)
nrow(nsv$nsv == 0)
```

#Data Modeling

The Random Forests algorithm was selected to generate the predictive model for its accuracy and is robust with regard to outliers in training data. A 5-fold cross validation was used. 

``` {r random forest}
#creating predictive model using random forests
control <- trainControl(method="cv", 5)
fit <- train(classe ~ ., data=myTraining, method="rf", trControl=control, ntree=250)
fit

#prediction on testing sub-dataset using model created
pred <- predict(fit, myTesting)
myTesting$predRight <- pred == myTesting$classe

#comparison of the prediction and the test sub-dataset, with accuracy and OOSE values
confusionMatrix(pred, myTesting$classe)

#accuracy of prediction model
accuracy <- postResample(pred, myTesting$classe)
accuracy

#estimated out-of-sample error
oose <- 1 - accuracy[[1]]
oose 
```

The accuracy of the predictive model was `r round(accuracy[[1]]*100,2)`% and the estimated out-of-sample error was `r round(oose*100,2)`%.

#Applying Predictive Model to 20 Test Cases

The predictive model was applied to the original testing dataset (20 test cases).

``` {r test case}
results <- predict(fit, testClean)
results
```
